\documentclass[a4paper,12pts]{article}

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}

\newcommand{\SAT}{\textnormal{$k$-SAT}}
\newcommand{\SATbf}{\textbf{$k$-SAT}}
\newcommand{\CNF}{\textnormal{$k$-CNF}}
\newcommand{\vbl}[1]{\textnormal{vbl(#1)}}
\newcommand{\dist}[2]{d_H(#1,#2)}
\newcommand{\ball}[2]{B_{#1}(#2)}
\newcommand{\ballk}[2]{B^k_{#1}(#2)}
\newcommand{\astar}{\alpha^*}
\newcommand{\PBS}{\textnormal{Promise-Ball-$\SAT$}}
\newcommand{\PBSbf}{\textbf{Promise-Ball-$\SATbf$}}
\newcommand{\cc}{\mathcal{C}}
\newcommand{\bits}{\{0,1\}}
\newcommand{\kbits}{\{1,...,k\}}
\newcommand{\poly}{\textnormal{poly}}
\renewcommand{\Pr}{\textnormal{Pr}}
\renewcommand{\O}{\mathcal{O}^*}

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem*{note}{Note}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}


\title{Explanatory Report on Full Derandomization of Sch\"{o}ning's $\SAT$ Algorithm}
\author{}
\date{}

\begin{document}

\maketitle

\section{Introduction}
\paragraph{} $\SAT$ is a well-known and probably the most studied NP-Complete problem. A naive search algorithm solves $\SAT$ in $\O(2^n)$ time by searching all possible assignments to all variables(we use $\O$ to suppress the polynomial factor in $n$). In 1999, Uwe Sch\"{o}ning gave a simple yet efficient randomized algorithm for $\SAT$\cite{Schoning99} that runs in $\O((2(1-\frac{1}{k}))^n)$. The algorithm was later derandomized in two subsequent papers\cite{Dantsin02,Moser11}. In this explanatory report, we will reorganize their results and present the fully derandomized algorithm.\par 
The explanatory report is organized in the following manner. In section 2, we introduce some basic notations and definitions used in this paper. In section 3, we present Sch\"{o}ning's original randomized algorithm\cite{Schoning99} for $\SAT$, reorganized to facilitate our analysis for derandomization. In section 4, a derandomization algorithm based on local search by \cite{Dantsin02} is given. In section 5, we present an improvement by \cite{Moser11} that improves the derandomization algorithm and attains performance arbitrarily close to the randomized version, thereby claiming a full derandomization.

\section{Preliminaries}
\paragraph{} An input instance of a $\SAT$ problem is a $\CNF$(Conjunctive Normal Form) formula $F$ with $n$ variables. $F$ contains a finite set of clauses where each clause $C$ contains at most $k$ pairwise independent literals. A literal $u$ is either a variable $x$ or a complemented variable $\bar{x}$. Let $V$ denote the set of variables in $F$. \par 
A truth assignment $\alpha: V\rightarrow \{0,1\}$ assigns each variable a boolean value. If a literal $u = x$, it is satisfied by $\alpha$ iff $\alpha(x) = 1$. If $u = \bar{x}$, it is satisfied by $\alpha$ iff $\alpha(x) = 0$. A clause $C$ is satisfied by $\alpha$ if at least one of its literals are satisfied by $\alpha$. A formula $F$ is satisfied by $\alpha$ if all clauses are satisfied by $\alpha$. Alternatively, for a fixed set of variables $V=\{x_1,x_2,...,x_n\}$, an assignment $\alpha$ can be considered as a $n$-bit binary string where the $i$th bit gives the value of $x_i$.\par 
Given two assignments $\alpha$ and $\beta$, the Hamming distance between them, denoted as $\dist{\alpha}{\beta}$, is defined to be the number of variables $x \in V$ where $\alpha(x) \neq \beta(x)$. A Hamming Ball centered at $\alpha$ with radius $r$ is defined to be $\ball{r}{\alpha} = \{\beta: \dist{\alpha}{\beta} \leq r\}$, that is, all assignments whose Hamming distance from $\alpha$ is less than or equals to $r$. For assignments of $n$ variables, let $V(r)$ denotes the volume of a Hamming Ball of radius $r$, that is, the number of assignments in the Hamming Ball.\par 
For an assignment $\alpha$ and a literal $l$, we use $\alpha^{[l = 1]}$ to denote the assignment that is derived from $\alpha$ after setting $l$ to be $1$. Similarly for a formula $F$, we use $F^{[l = 1]}$ to denote the new formula obtained after setting $l$ to be $1$. \par 
\begin{definition}[\PBS\cite{Moser11}]
	An instance of the $\PBS$ problem $(F,\alpha,r)$ consists of a $\CNF$ formula $F$, an assignment $\alpha$ for $F$ and a natural number $r$. With the promise that the Hamming Ball $\ball{r}{\alpha}$ contains a satisfying assignment for $F$. Find any satisfying assignment.
\end{definition}
The $\PBS$ problem is an important sub-problem for solving $\SAT$. 

\section{Sch\"{o}ning's $\SATbf$ Algorithm\cite{Schoning99}}
The randomized algorithm for $\SAT$ builds on a simple random walk algorithm for solving $\PBS$: \par 
\begin{algorithm}[H]
	\caption{$\PBS$-Random-Walk}
	\DontPrintSemicolon
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
	\Input{$(F,\alpha,r)$}
	\Output{A satisfying assignment $\astar$ or NO if no $\astar$ are found}
	\BlankLine
	\For{steps = 1 to 3n}{
		\lIf{$\alpha$ satisfies $F$}{return $\alpha$}
		Choose an arbitrary clause $C$ not satified by $\alpha$\\
		Choose a literal in $C$ uniformly at random and flip its value in $\alpha$
	}
	\lIf{$\alpha$ satisfies $F$}{return $\alpha$}
	\hspace{7em}\lElse{return NO}
\end{algorithm}

\begin{lemma}[\cite{Schoning99}]
For a satisfying assignment $\astar \in \ball{r}{\alpha}$, where $\dist{\alpha}{\astar} = d \leq r$, $\PBS \textnormal{-Random-Walk}$ returns $\astar$ with probability $\geq (\frac{1}{k-1})^d$.
\end{lemma}
\begin{proof}[Proof Sketch]
	Let $q_d$ be the probability that $\PBS$-Random-Walk returns a satisfying assignment for $F$ given $\dist{\astar}{\alpha} = d$. In each step, with probability $\geq \frac{1}{k}$ the assignment $\alpha$ goes one step closer to $\astar$ and with probability $\leq 1 - \frac{1}{k}$ it goes one step further from $\astar$. Hence, $q_d \geq $ the probability that the random walk starting at position $d$ reaches position $0$ within $3n$ steps. Detailed analysis of the random walk gives $q_d \geq (\frac{1}{k-1})^d$.
\end{proof}

%Do we need this?
\iffalse
\begin{note}
	The input parameter $r$ is not used in $\PBS$\textnormal{-Random-Walk}. By nature of randomness, $\PBS$\textnormal{-Random-Walk} solves $(F,\alpha,d)$ with probability $\geq (\frac{1}{k-1})^{d}$.
\end{note}
\fi

With $\PBS$-Random-Walk, we have a randomized algorithm for $\SAT$:\par 
\begin{algorithm}[H]
	\caption{Randomized $\SAT$}
	\DontPrintSemicolon
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
	\Input{A $\CNF$ formula $F$ with $n$ variables}
	\Output{A satisfying assignment $\astar$ or NO if no $\astar$ are found}
	\BlankLine
	\For{iterations = 1 to m}{
		Choose a assignment $\alpha$ for $F$ uniformly at random\\
		$\astar$ = $\PBS$-Random-Walk($F,\alpha,n$)\\
		\lIf{$\astar$ satisfies $F$}{return $\astar$}
	}
	{return NO}
\end{algorithm}
\begin{lemma}[\cite{Schoning99}]
	If $F$ is satisfiable, each iteration of the for-loop in \textnormal{Randomized-}$\SAT$ returns a satisfying assignment with probability $\geq (\frac{k}{2(k-1)})^n$
\end{lemma}
\begin{proof}
	Fix a satisfying assignment $\astar$ for $F$, for the random initial assignment $\alpha$, $\Pr(\dist{\astar}{\alpha}=d) = {n \choose d}{2}^{-n}$. With \textbf{Lemma 1}, we have:
	\begin{align*}
	&\Pr(\textnormal{Success in an iteration}) 
	\geq \sum_{d=0}^{n} {n \choose d} 2^{-n} \cdot \big(\frac{1}{k-1}\big)^{d} 
	\geq \big(\frac{1}{2}\big)^{n} (1 + \frac{1}{k-1}) ^n = \big(\frac{k}{2(k-1)}\big)^{n}
	\end{align*}
\end{proof}
We can then amplify the probability by setting the number of iterations $m = \O((\frac{2(k-1)}{k})^n) = \O\big((2(1-\frac{1}{k}))^n\big)$. Each iteration clearly runs in polynomial time and the overall runtime for the randomized algorithm is $\O\big((2(1-\frac{1}{k}))^n\big)$.
\section{Derandomization with Local Search\cite{Dantsin02}}
\paragraph{} Notice that in the randomized algorithm, randomness is used in two parts. One is in $\PBS$-Random-Walk and the other one is to find an assignment $\alpha$ that serves as the center of Hamming Ball $\ball{r}{\alpha}$. Both of them needs to be derandomized. \par 
\subsection{Derandomization of {$\PBSbf$}}
First of all, we derandomize $\PBS$-Random-Walk with a simple recursion algorithm. \par 
\begin{algorithm}[H]
	\caption{$\PBS$-Recursion}
	\DontPrintSemicolon
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
	\Input{$(F,\alpha,r)$}
	\Output{A satisfying assignment $\astar$ or NO if no $\astar$ are found}
	\BlankLine
	\lIf{$\alpha$ satisfies $F$}{return $\alpha$}
	\hspace{7em}\lElseIf{r = 0}{return NO}
	Choose an arbitrary clause $C$ not satified by $\alpha$\\
	\ForEach{literal $l \in C$}{
		$\alpha'$ = $\PBS$-Recursion($F^{[l = 1]}, \alpha^{[l=1]},r-1)$\\
		\lIf {$\alpha'$ satisfies $F$}{Return $\alpha'$}
	}
	{return NO}
\end{algorithm}
\begin{lemma}
	$\PBS \textnormal{-Recursion}$ solves $\PBS$ in $\O(k^r)$ time.
\end{lemma}
\begin{proof}
	The running time is straightforward. Each clause has at most $k$ literals and hence each node in the recursion tree has at most $k$ branches. The depth of the recursion three is at most $r$. Hence, there are at most $k^r$ leaves in the recursion tree. The overall runtime is $\O(k^r)$. \par 
	The correctness can be seen by drawing analogy to the random walk algorithm. Let $\astar$ be the satisfying assignment such that $\dist{\astar}{\alpha} \leq r$. Then there exists a ``lucky" random walk from $\alpha$ to $\astar$ that takes exactly $\dist{\astar}{\alpha}$ steps, that is, in each step we move one step closer to $\astar$ and never makes a wrong move. Notice that if we never makes a wrong move, we will never change the assignment of a variable for more than once. \par 
	Now consider the recursion algorithm, in each recursive step we choose a clause $C$ not satisfied by $\alpha$ and we know $C$ is satisfied by $\astar$. Hence, at least one of the literals in $C$ has a different value from $\astar$. Now instead of guessing randomly which is the right literal to flip as in the random walk algorithm, we try all possibilities and at least one of them would be a correct move, giving us $\dist{\astar}{\alpha^{[l = 1]}} = r - 1$. Moreover, if we make a good move, we do not need to change the assignment of that literal in the future. We can then assign that value permanently and search using $F^{[l=1]}$ and $\alpha^{[l=1]}$. Hence, this is a depth-first-search algorithm that searches for the ``lucky" path from $\alpha$ to $\astar$.
\end{proof}

\begin{note}
	$\PBS$-Recursion is faster than enumerating all elements in the Hamming Ball because we are indeed performing an informed search. We only look at unsatisfied clauses and hence prune out many unnecessary branches.
\end{note}
\subsection{Derandomization Using Covering Code}
\paragraph{} Next, we derandomize the choices of initial assignment $\alpha$ with Covering Code. \par 
\begin{definition}[Covering Code]
	A (binary) Covering Code of length $n$ and radius $r$, $\cc_r \subseteq \bits^n$ is defined as: $\forall \alpha \in \bits^n, \exists c \in \cc_r$ such that $\alpha \in \ball{r}{c}$. That is, the Hamming Balls of radius $r$ centered at each member of $\cc_r$ covers the set $\bits^n$.
\end{definition}
\begin{lemma}[Bounding the volume of a Hamming Ball of radius $r$\cite{Dantsin02}]
	\begin{align*}
	V(r) = \sum_{i = 0}^{n} {n \choose i}
	\end{align*}
	Let $\rho = r/n$. If $0 < \rho \leq 1/2$, then $V(r)$ can be estimated as follows, cf. \cite[Lemma 2.4.4]{Cohen97} 
	\begin{align*}
	\frac{1}{\sqrt{8n\rho(1-\rho)}} \cdot 2^{h(\rho)n} \leq V(r) \leq 2^{h(\rho)n}
	\end{align*}
	where $h(\rho) = -\rho \log_2 \rho - (1-\rho)\log_2(1-\rho)$ is the binary entropy function.
\end{lemma}
\begin{lemma}[\cite{Dantsin02}]
	There exists a covering code $\cc_r$ of length $n$ and its size $|\cc_r|$ is at most $\lceil n\cdot 2^n / V(r) \rceil$.
\end{lemma}
\begin{proof}
	We give a probabilistic argument. Suppose we randomly choose $ n\cdot 2^n / V(r)$ assignments from $\bits^n$ uniformly and independently to form $\cc_r$. Now fixed an assignment $\alpha$. The probability that $\alpha$ is covered by the Hamming Ball centered at a randomly chosen member $c \in \cc_r$ is exactly $V(r)/2^n$. 
	\begin{align*}
	\Pr(\text{$\alpha$ is not covered}) = \Big(1-\frac{V(r)}{2^n}\Big)^{n\cdot \frac{2^n}{V(r)}} \leq e^{-n}
	\end{align*}
	By taking a union bound over all $2^n$ possible assignments, the probability that $\cc_r$ is a covering code $\geq 1 - 2^n\cdot e^{-n} > 0$. Hence, such a covering code exists. 
\end{proof}
Combining \textbf{Lemma 4} and \textbf{Lemma 5}, we have the following corollary:
\begin{corollary}
	Let $0 < \rho \leq 1/2$ and let $\beta(n) = \sqrt{8n\rho(1-\rho)}$. For all $n$, there exists a covering code $\cc_{\rho n}$ of length $n$, radius $\rho n$ and size at most $n\beta(n)\cdot 2^{(1-h(\rho))n}$.
\end{corollary}
\begin{lemma}[\cite{Dantsin02}]
	A covering code of length $n$ and size at most $n^2\beta(n)\cdot 2^{(1-h(\rho))n}$ can be constructed in $\O(2^{3n})$ time.
\end{lemma}
\begin{proof}
	Notice that constructing a covering code is indeed a set cover problem with $2^n$ sets and $2^n$ elements. There is a well-known greedy algorithm that gives a $\log(m)$-approximation\cite{Hochbaum97} to the set cover problem. In each iteration, we pick the Hamming Ball that covers the largest number of not-yet-covered elements. We can associate each Hamming Ball with the number of not-yet-covered elements that it covers. In each iteration, we pick a Hamming Ball greedily and update the numbers for all remaining Balls. This takes $\poly(n) \cdot 2^n \cdot 2^n = \poly(n) \cdot 2^{2n}$ time. There are at most $2^n$ iterations. Hence, the overall running time is $\O(2^{3n})$. \par 
	From \textbf{Corollary 1}, we know the optimal size of the covering code is at most $n\beta(n)\cdot 2^{(1-h(\rho))n}$. The algorithm above gives a $\log(m)$ approximation. Note that the size of the problem $m = 2^n$. Hence, the size of the covering code constructed is at most $\log(2^n) \cdot n\beta(n)\cdot 2^{(1-h(\rho))n} = n^2\beta(n)\cdot 2^{(1-h(\rho))n}$
\end{proof}
\begin{lemma}
	Let $d\geq 2$ be a divisor of $n$ and $0<\rho \leq 1/2$. Then a covering code of length $n$ and size at most $\O(2^{(1-h(\rho))n})$ can be constructed in time $\O(2^{3n/d}+ 2^{(1-h(\rho))n})$.
\end{lemma}
\begin{proof}
	Using \textbf{Lemma 6}, we can construct a covering code $\cc'$ of length $\frac{n}{d}$, radius $\frac{\rho n}{d}$. Now define $\cc = \{c_1c_2...c_d:c_1,c_2,...,c_d \in \cc'\}$ to be the covering code of length $n$, radius $\rho n$, that is, $\cc$ is the set of all concatenations of $d$ codes from $\cc'$.\par 
	It is easy to verify that $\cc$ is a covering code of length $n$. For any $\alpha \in \bits^n$, split $\alpha$ into $d$ blocks of length $\frac{n}{d}$ i.e. $\alpha = \alpha_1\alpha_2...\alpha_d$. For each $\alpha_i$, there exists a code $c_i \in \cc'$ such that $\dist{\alpha_i}{c_i} \leq \frac{\rho n}{d}$. Now the code $c = c_1c_2...c_d \in \cc$ and $\dist{\alpha}{c} \leq \rho n$. \par 
	The size of $\cc'$ is at most $n^2\beta(n)\cdot 2^{(1-h(\rho))n/d}$ and constructing it takes $\O(2^{3n/d})$ time. The size of $\cc = |\cc'|^d \leq n^{2d}\beta(n)^d\cdot 2^{(1-h(\rho))n} = \O(2^{(1-h(\rho))n})$. Hence, the time taken to construct $\cc$ is $\O(2^{3n/d}+ 2^{(1-h(\rho))n})$.
\end{proof}
\subsection{Combining the Derandomization}
\paragraph{}Now that we have a covering code, we can complete our deterministic algorithm for $\SAT$:\par 
\begin{algorithm}[H]
	\caption{Deterministic $\SAT$}
	\DontPrintSemicolon
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
	\Input{A $\CNF$ formula $F$ with $n$ variables}
	\Output{A satisfying assignment $\astar$ or NO if no $\astar$ are found}
	\BlankLine
	Initialization: Compute covering code $\cc_{\rho n}$\\
	\ForEach{code $c \in \cc_{\rho n}$}{
		$\astar$ = $\PBS$-Recursion($F,c,\rho n$)\\
		\lIf{$\astar$ satisfies $F$}{return $\astar$}
	}
	{return NO}
\end{algorithm}
It is easy to see that Deterministic $\SAT$ returns a satisfying assignment if $F$ is satisfiable. Consider a satisfying assignment $\astar$. By definition of the covering code, $\exists c \in \cc_{\rho n}$ such that $\dist{c}{\astar} \leq \rho n$. Hence, the depth-first-search $\PBS$-Recursion($F,c,\rho n$) would be able to find $\astar$. \par
The running time $T(n,\rho,d)$ involves computing the covering code,  $\O(2^{3n/d}+ 2^{(1-h(\rho))n})$ and doing the search, $\O(2^{(1-h(\rho))n})\cdot \O(k^{\rho n})$. That is,
\begin{align*}
T(n,\rho,d) = \O(2^{3n/d}+ 2^{(1-h(\rho))n}) + \O(2^{(1-h(\rho))n})\cdot \O(k^{\rho n})
\end{align*} 
It is easy to see that for a reasonably large $d$ such as $d \geq 6$, the first term is completely dominated by the second term. By taking partial derivatives about $\rho$ on the exponential term, we can easily find out $\rho = 1/(k+1)$ minimized the running time. Hence, 
\begin{align*}
T(n,\rho,d) &=\O(2^{(1-h(\rho))n} \cdot k^{\rho n})\\
&= \O\Big(2^{(1+ \frac{1}{k+1}\log_2\frac{1}{k+1} + \frac{k}{k+1}\log_2\frac{k}{k+1})n} \cdot 2^{(\frac{1}{k+1} \log_2 k )n}\Big)\\
&= \O\Big(2^{(1 - \frac{1}{k+1}\log_2 (k+1) + \frac{k}{k+1}\log_2 k - \frac{k}{k+1}\log_2 (k+1) + \frac{1}{k+1} \log_2 k)n}\Big)\\
&= \O\Big(2^{(1 - \log_2 (k+1) + \log_2 k)n}\Big)\\
&= \O\big(\big(2 \cdot \frac{k}{k+1}\big)^{n}\big) = \O\big((2(1 - \frac{1}{k+1}))^n\big)
\end{align*} 
Notice that the performance is worse than the randomized algorithm and we will see how to improve it in the next section. In particular, we observe that the parameter $k$ in the overall running time comes from the running time of $\PBS$-Recursion, $\O(k^{\rho n})$. If we can improve its performance to $\O((k-1)^{\rho n})$, we arrive at a $\O\big((2(1 - \frac{1}{k}))^n\big)$ running time which is on par with the randomized algorithm.
\section{Improving on the Derandomization\cite{Moser11}}
\paragraph{} Notice that in the previous section, instead of searching the whole space of possible assignments, we cover the whole space with a number of Hamming Balls and search in each Hamming Ball. The same idea can be used when searching in a Hamming Ball. In particular, we try to cover a large Hamming Ball with smaller Hamming Balls while attempting the reduce the number of literals in the clauses. By repeating this process recursively, we arrive at a more efficient algorithm for $\PBS$. First of all, we look at $k$-nary covering code which is similar to the binary covering code in section 4.2.
\subsection{k-nary Covering Code}
\paragraph{}Consider the set $\kbits^t$, for any element $w,w' \in \kbits^t$, let $w_i$ and $w'_i$ denote the $i$th coordinate of $w$ and $w'$ respectively. The Hamming distance $\dist{w}{w'}$ is similarly defined to be the number of coordinates that $w$ and $w'$ do not agree, that is, $\dist{w}{w'} = |\{i:w_i \neq w'_i, 1\leq i \leq t \}|$. Also we can define $k$-Hamming Balls $\ballk{r}{w} = \{w':\dist{w}{w'} \leq r\}$, that is, all elements whose Hamming distance from $w$ is at most $r$. 
\begin{lemma}[Bounding the volume of a $k$-Hamming Ball of radius r\cite{Moser11}]
	Let $V^k(r)$ denotes the volume of a $k$-Hamming Ball centered at $w$. Consider the set $S = \{w' \in \kbits^t: \dist{w}{w'} = r\}$. There are ${t \choose r}$ possible ways to choose the coordinates that are different in $w$ and $w'$. Each coordinate has $k-1$ choices of value. Hence,
	\begin{align*}
	V^k(r) > |S| \geq {t \choose r}(k-1)^r
	\end{align*}
\end{lemma}
\begin{definition}[k-nary Covering Code]
	A k-nary Covering Code of length $t$ and radius $r$, $\cc_r^k \subseteq \kbits^t$ is defined as $\forall w \in \kbits^t, \exists c \in \cc_r^k$ such that $w \in \ballk{r}{c}$. That is, the Hamming Balls of radius $r$ centered at each member of $\cc_r^k$ covers the set $\kbits^t$. 
\end{definition}
\begin{lemma}
	For any $t,k \in \mathbb{N}$, there exists a k-nary covering code $\cc^k_r \in \kbits^t$ of radius $r$ such that 
	\begin{align*}
	|\cc^k_r| \leq \left \lceil \frac{t\ln(k)k^t}{{t \choose r}(k-1)^r} \right \rceil
	\end{align*}
\end{lemma}
\begin{proof}
	The proof is similar to the probabilistic proof for \textbf{Lemma 5}. Suppose we randomly choose $\frac{t\ln(k)k^t}{{t \choose r}(k-1)^r}$ members from $\kbits^t$ uniformly and independently to form $\cc^k_r$. Now fixed an member $w \in \kbits^t$. The probability that $w$ is covered by the Hamming Ball centered at a randomly chosen member $c \in \cc^k_r$ is exactly $V^k(r)/k^t$. 
	\begin{align*}
	\Pr(\text{$w$ is not covered}) &= \Bigg(1-\frac{V^k(r)}{k^t}\Bigg)^{\frac{t\ln(k)k^t}{{t \choose r}(k-1)^r}}\\
	&<\Bigg(1-\frac{{t \choose r}(k-1)^r}{k^t}\Bigg)^{\frac{t\ln(k)k^t}{{t \choose r}(k-1)^r}}\\
	&\leq e^{-t\ln(k)} \leq k^{-t}
	\end{align*}
	By taking a union bound over all $k^t$ members from $\kbits^t$, the probability that $\cc^k_r$ is a covering code $> 1 - k^t\cdot k^{-t} > 0$. Hence, such a covering code exists. 
\end{proof}
We then try to approximate the size of the $k$-nary covering code with the following bound:
\begin{lemma}[\cite{MacWilliams77}]
	For $0<\rho \le 1/2$ and $t \in \mathbb{N}$, it holds that
	\begin{align*}
	{t \choose \rho t}\geq \frac{1}{\sqrt{8t\rho(1-\rho)}}\Big(\frac{1}{\rho}\Big)^{\rho t}\Big(\frac{1}{1-\rho}\Big)^{(1-\rho) t}
	\end{align*} 
	Choose $\rho = \frac{1}{k}$, we have
	\begin{align*}
	{t \choose t/k}\geq \frac{1}{\sqrt{8t}}(k)^{t/k}\Big(\frac{k}{k-1}\Big)^{(k-1) t/k} = \frac{k^t}{\sqrt{8t}(k-1)^{(k-1)t/k}}
	\end{align*} 
\end{lemma}
	Combining \textbf{Lemma 9} and \textbf{Lemma 10}, by choosing $r = \rho t = \frac{t}{k}$, for sufficiently large $t$, we can approximate $|\cc^k_{t/k}|$ by
\begin{align*}
|\cc^k_{{t/k}}| \leq \left \lceil \frac{t\ln(k)k^t}{{t \choose t/k}(k-1)^{t/k}} \right \rceil \leq \frac{t^2 k^t (k-1)^{(k-1)t/k}}{k^t(k-1)^{(k-1)t/k}} \leq t^2(k-1)^{t-2t/k}
\end{align*}
\subsection{Faster $\PBSbf$ Algorithm}
Using the previous tool, we are now ready to present the derandomised version of Sch\"{o}ning's algorithm. At first, before doing anything else, the algorithm will firstly compute a code $\mathcal{C} \subseteq {1, ...,k}^t$, where each element $c \in \mathcal{C}$ has a radius of $\frac{t}{k}$, where $t$ is a some constant.\par

Upon the input of some k-CNF, the algorithm greedily tries to find the maximal set of clauses that are independent of each other. Let's call this set $G = \{\mathcal{C}_1, \mathcal{C}_2, ...,\mathcal{C}_m\}$. Now we can break the problem instance into 2 cases: \par

Base case: The number of independent clauses is less than $t$. In which case we enumerate all possible assignments (of which there are $2^{km}$) and then call algorithm 3 with that assignment, with a radius of $r$. Essentially, this is just a search around all the possible assignments of distance at most $r$ away. Since we've set this to only happen when the number of clauses is below a certain constant, we have the number of possible assignments, $2^{km}$ of them is actually also just a constant. And since each search is bounded by $\O(k^r)$ time, we have that the base case runs in the same amount of time, since the search is only called a constant number of times.\par

On the other hand, if the number of independent clauses is more than $t$. Then the algorithm should select a set $H \subseteq G$ such that $H$ is of size exactly $t$. Then, we will use the previously constructed k-ary covering code to decide how to choose assignments for this set of clauses $H$.\par

To do this, let $w \in \{1, ..., k\}^t$, so $w = (w_1, w_2, ..., w_t)$ where each $w_i$ would be within the range of $1$ to $k$. We will assign the $w_i^{th}$ literal in the $i^{th}$ clause in $H$ to be satisfied. We shall denote this change in assignment to be $\alpha[w]$ from some initial assignment $\alpha$. Note that we need to fix some arbitrary ordering on the clauses. \par

What the algorithm would do, is recursively try to solve the resulting formula obtained after trying all the $w \in \mathcal{C}$. 

We also need to set a limit of the radius of how far we are to deviate from our original assignment. This bound is necessary for the running time of the algorithm.\par

Notice that the covering code $\mathcal{C}$ that was computed so that each $w \in \mathcal{C}$ has a radius of $\frac{t}{k}$. What this means is that assuming some satisfying assignment $\alpha^*$ exists, letting $\alpha$ be some assignment that leaves some $t$ clauses unsatisfied and $w^*$ be some coordinate that corresponds to the satisfying assignment $\alpha$, there exists an element of the covering code $w' \in \mathcal{C}$ that is at most $\frac{t}{k}$ away from it. Thus $
alpha[w']$ should in the worst case increase the distance by at most $\frac{t}{k}$ whereas decrease the distance by at least $t - \frac{t}{k}$. Thus, we have that at each change in our assignment for the formula: \par

\begin{center}
	$d_H(\alpha[w'], \alpha^{*}) \leq d_H(\alpha, \alpha^{*}) + \frac{t}{k} - (t - \frac{t}{k}) \leq r - (t - \frac{2t}{k})$
\end{center}

Essentially this implies every time we apply $w$ onto $\alpha$, the satisfying assignment should be that much closer.\par

\begin{algorithm}[H]
	\caption{ Faster-$\PBSbf$ Algorithm}
	\DontPrintSemicolon
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
	\Input{A $\CNF$ formula $F$ with $n$ variables, assignment $\alpha$, radius $r$, $\mathcal{C} \subseteq \{1,...,k\}^t$}
	\Output{A satisfying assignment $\astar$ or NO if no $\astar$ are found}
	\BlankLine
	\lIf{$\astar$ satisfies $F$}{return $\astar$}
	\lElseIf{$r = 0$} {return $false$}
	\BlankLine
	Find $G$ greedily as the maximal set of pairwise disjoint k-clauses from $F$ unsatisfied by $\alpha$\\
	\If{$|G| < t$}{ 
		\ForEach{possible assignment $\beta$ to the variables in $G$}{
			\lIf{Recursion($F^{[\beta]}$, $\alpha$, $r$) = true}{return true}
		}
	}		
	\Else {Let $H \subseteq G$ such that $|H| = t$
	\ForEach{$w \in \mathcal{C}$} 
	{
		\lIf{Faster-$\PBSbf$(F, $\alpha[H, w]$, $r - (t - \frac{2t}{k})$) = true} {return true}
	}
	}
	\end{algorithm}

\subsubsection{Running time analysis}
To bound the number of leaves in this recursion tree, we note that at each level we make $\mathcal{C}$ calls and $r$ decreases by $t-\frac{2t}{k}$. Letting $\delta = t-\frac{2t}{k}$. Thus: 
\begin{align*} 
\mathcal{C}^{r/\delta} &\leq (t^{2}(k-1)^{\delta})^{r/\delta}\\ 
&= ((k-1)t^{2/\delta})^{r} 
\end{align*} 
And since $t$ is some constant independent of the problem size, the running time is just: $(k-1)^{r}$. 

\begin{thebibliography}{9}
	
\bibitem{Cohen97}
G. Cohen, I. Honkala, S. Litsyn and A. Lobstein.
\textit{Covering Codes.}
Mathematical Library, vol. 54, Elsevier, Amsterdam, (1997).
\bibitem{Dantsin02}
E. Dantsin, A. Goerdt, E. A. Hirsch, R. Kannan, J. Kleinberg, C. Papadimitriou, O. Raghavan, and U. Sch\"{o}ning. 
\textit{A deterministic $(2-2/(k+1))n$ algorithm for k-SAT based on local search.} Theoretical Computer Science 289, (2002).

\bibitem{Hochbaum97}
D.S. Hochbaum (Ed.).
\textit{Approximation Algorithms for NP-hard Problems.}
PWS Publishing Company, MA, (1997).

\bibitem{MacWilliams77}
F. J. MacWilliams and N. J. A. Sloane.
\textit{The theory of error-correcting codes. II.}
North-Holland Publishing Co., Amsterdam. North-Holland Mathematical Library, Vol. 16, (1977).

\bibitem{Moser11}
R. Moser and D. Scheder.
\textit{A Full Derandomization of Sch\"{o}ningâ€™s k-SAT Algorithm.} Proceedings of the 44th Annual ACM Symposium on Theory of Computing, (2011).

\bibitem{Schoning99}
U. Sch\"{o}ning.
\textit{A Probabilistic Algorithm for $k$-SAT and Constraint Satisfaction Problems.} Proceedings of the 40th Annual Symposium on Foundations of Computer Science, (1999).

\end{thebibliography}

\end{document}

