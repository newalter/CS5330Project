\documentclass[a4paper,12pts]{article}

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}

\newcommand{\SAT}{\textnormal{$3$-SAT}}
\newcommand{\SATbf}{\textbf{$3$-SAT}}
\newcommand{\CNF}{\textnormal{$3$-CNF}}
\newcommand{\vbl}[1]{\textnormal{vbl(#1)}}
\newcommand{\dist}[2]{d_H(#1,#2)}
\newcommand{\ball}[2]{B_{#1}(#2)}
\newcommand{\astar}{\alpha^*}
\newcommand{\PBS}{\textnormal{Promise-Ball-$\SAT$}}
\newcommand{\PBSbf}{\textbf{Promise-Ball-$\SATbf$}}
\newcommand{\cc}{\mathcal{C}}
\newcommand{\bits}{\{0,1\}}
\newcommand{\poly}{\textnormal{poly}}
\renewcommand{\Pr}{\textnormal{Pr}}
\renewcommand{\O}{\mathcal{O}^*}

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem*{note}{Note}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}


\title{Explanatory Report on Full Derandomization of Sch\"{o}ning's $\SAT$ Algorithm}
\author{}
\date{}

\begin{document}

\maketitle

\section{Introduction}
\paragraph{} The explanatory report is organized in the following manner. In section 2, we introduce some notations and definitions used in this paper. In section 3, we present Sch\"{o}ning's original randomized algorithm\cite{Schoning99} for $\SAT$. In section 4, a derandomization algorithm based on local search by \cite{Dantsin02} is given. In section 5, we present an improvement by \cite{Moser11} that builds on the derandomization algorithm whose performance is arbitrarily close to the randomized version, thereby claiming a full derandomization.

\section{Preliminaries}
\paragraph{} An input instance of a $\SAT$ problem is a $\CNF$(Conjunctive Normal Form) formula $F$ with $n$ variables. $F$ contains a finite set of clauses where each clause $C$ contains at most $3$ pairwise independent literals. A literal $u$ is either a variable $x$ or a complemented variable $\bar{x}$. Let $V$ denote the set of variables in $F$. \par 
A truth assignment $\alpha: V\rightarrow \{0,1\}$ assigns each variable a boolean value. If a literal $u = x$, it is satisfied by $\alpha$ iff $\alpha(x) = 1$. If $u = \bar{x}$, it is satisfied by $\alpha$ iff $\alpha(x) = 0$. A clause $C$ is satisfied by $\alpha$ if at least one of its literals are satisfied by $\alpha$. A formula $F$ is satisfied by $\alpha$ if all clauses are satisfied by $\alpha$. \par 
Given two assignment $\alpha$ and $\beta$, the hamming distance between them $\dist{\alpha}{\beta}$ is defined to be the number of variables $x \in V$ where $\alpha(x) \neq \beta(x)$. A Hamming Ball centered at $\alpha$ with radius $r$ is defined to be $\ball{r}{\alpha} = \{\beta: \dist{\alpha}{\beta} \leq r\}$ that is, all assignments whose hamming distance from $\alpha$ is less or equals to $r$. For assignments of $n$ variables, let $V(r)$ denotes the volume of a Hamming Ball of radius $r$, that is, the number of assignments in the Hamming Ball.\par 
For an assignment $\alpha$ and a literal $l$, we use $\alpha^{[l = 1]}$ to denote the assignment that derives from $\alpha$ after setting $l$ to be $1$. Similarly for a formula $F$, we use $F^{[l = 1]}$ to denote the new formula after setting $l$ to be $1$. \par 
\begin{definition}[\PBS\cite{Moser11}]
	An instance of the $\PBS$ problem $(F,\alpha,r)$ consists of a $\CNF$ formula $F$, an assignment $\alpha$ for $F$ and a natural number $r$. With the promise that the Hamming Ball $\ball{r}{\alpha}$ contains a satisfying assignment for $F$. Find any satisfying assignment.
\end{definition}
The $\PBS$ problem is an important sub-problem for solving $\SAT$. 

\section{Sch\"{o}ning's $3$-SAT Algorithm\cite{Schoning99}}
The randomized algorithm for $\SAT$ builds on a simple random walk algorithm for solving $\PBS$: \par 
\begin{algorithm}[H]
	\caption{$\PBS$-Random-Walk}
	\DontPrintSemicolon
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
	\Input{$(F,\alpha,r)$}
	\Output{A satisfying assignment $\astar$ or NO if no $\astar$ are found}
	\BlankLine
	\For{steps = 1 to 3n}{
		\lIf{$\alpha$ satisfies $F$}{return $\alpha$}
		Choose an arbitrary clause $C$ not satified by $\alpha$\\
		Choose a literal in $C$ uniformly at random and flip its value in $\alpha$
	}
	\lIf{$\alpha$ satisfies $F$}{return $\alpha$}
	\hspace{7em}\lElse{return NO}
\end{algorithm}

\begin{theorem}[\cite{Schoning99}]
For a satisfying assignment $\astar \in \ball{r}{\alpha}$, where $\dist{\alpha}{\astar} = d \leq r$, $\PBS \textnormal{-Random-Walk}$ returns $\astar$ with probability $\geq (\frac{1}{2})^d$.
\end{theorem}
\begin{proof}[Proof Sketch]
	Let $q_d$ be the probability that $\PBS$-Random-Walk returns a satisfying assignment for $F$ given $\dist{\astar}{\alpha} = d$. In each step, with probability $\geq \frac{1}{3}$ the assignment $\alpha$ goes one step closer to $\astar$ and with probability $\leq \frac{2}{3}$ it goes one step further from $\astar$. Hence, $q_d \geq $ the probability that the random walk starting at position $r$ reaches position $0$ within $3n$ steps. Detailed analysis of the random walk gives $q_d \geq (\frac{1}{2})^d$.
\end{proof}

%Do we need this?
\iffalse
\begin{note}
	The input parameter $r$ is not used in $\PBS$\textnormal{-Random-Walk}. By nature of randomness, $\PBS$\textnormal{-Random-Walk} solves $(F,\alpha,d)$ with probability $\geq (\frac{1}{2})^{d}$.
\end{note}
\fi

With $\PBS$-Random-Walk, we have a randomized algorithm for $\SAT$:\par 
\begin{algorithm}[H]
	\caption{Randomized $\SAT$}
	\DontPrintSemicolon
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
	\Input{A $\CNF$ formula $F$ with $n$ variables}
	\Output{A satisfying assignment $\astar$ or NO if no $\astar$ are found}
	\BlankLine
	\For{iterations = 1 to m}{
		Choose a assignment $\alpha$ for $F$ uniformly at random\\
		$\astar$ = $\PBS$-Random-Walk($F,\alpha,n$)\\
		\lIf{$\astar$ satisfies $F$}{return $\astar$}
	}
	{return NO}
\end{algorithm}
\begin{theorem}
	If $F$ is satisfiable, each iteration of the for-loop in \textnormal{Randomized-}$\SAT$ returns a satisfying assignment with probability $\geq (\frac{3}{4})^n$
\end{theorem}
\begin{proof}
	Fix a satisfying assignment $\astar$ for $F$, for the random initial assignment $\alpha$, $\Pr(\dist{\astar}{\alpha}=d) = {n \choose d}{2}^{-n}$. With \textbf{Theorem 1}, we have:
	\begin{align*}
	&\Pr(\textnormal{Success in one iteration}) 
	\geq \sum_{d=0}^{n} {n \choose d} 2^{-n} \cdot \big(\frac{1}{2}\big)^{d} 
	\geq \big(\frac{1}{2}\big)^{n} (1 + \frac{1}{2}) ^n = \big(\frac{3}{4}\big)^{n}
	\end{align*}
\end{proof}
We can then amplify the probability by setting the number of iterations $m = \O((\frac{4}{3})^n)$(we use $\O$ to suppress the polynomial factor in $n$). Each iteration clearly runs in polynomial time and the overall runtime for the randomized algorithm is $\O((\frac{4}{3})^n)$.
\section{Derandomization with Local Search\cite{Dantsin02}}
\paragraph{} Notice that in the randomized algorithm, randomness is used in two parts. One is in $\PBS$-Random-Walk and the other one is to find an assignment $\alpha$ that serves as the center of Hamming Ball $\ball{r}{\alpha}$. Both of them needs to be derandomized. \par 
\subsection{Derandomization of {$\PBSbf$}}
First of all, we derandomize $\PBS$-Random-Walk with a simple recursion algorithm. \par 
\begin{algorithm}[H]
	\caption{$\PBS$-Recursion}
	\DontPrintSemicolon
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
	\Input{$(F,\alpha,r)$}
	\Output{A satisfying assignment $\astar$ or NO if no $\astar$ are found}
	\BlankLine
	\lIf{$\alpha$ satisfies $F$}{return $\alpha$}
	\hspace{7em}\lElseIf{r = 0}{return NO}
	Choose an arbitrary clause $C$ not satified by $\alpha$\\
	\ForEach{literal $l \in C$}{
		$\alpha'$ = $\PBS$-Recursion($F^{[l = 1]}, \alpha^{[l=1]},r-1)$\\
		\lIf {$\alpha'$ satisfies $F$}{Return $\alpha'$}
	}
	{return NO}
\end{algorithm}
\begin{theorem}
	$\PBS \textnormal{-Recursion}$ solves $\PBS$ in $\O(3^r)$ time.
\end{theorem}
\begin{proof}
	The running time is straightforward. Each clause has at most $3$ literals and hence each node in the recursion tree has at most $3$ branches. The depth of the recursion three is at most $r$. Hence, there are at most $3^r$ leaves in the recursion tree. The overall runtime is $\O(3^r)$. \par 
	The correctness can be seen by drawing analogy to the random walk algorithm. Let $\astar$ be the satisfying assignment such that $\dist{\astar}{\alpha} \leq r$. Then there exists a ``lucky" random walk from $\alpha$ to $\astar$ that takes $\leq r$ steps, that is, in each step we move one step closer to $\astar$ and never makes a wrong move. Notice that if we never makes a wrong move, we will never change the assignment of a variable for more than once. \par 
	Now consider the recursion algorithm, in each recursive step we choose a clause $C$ not satisfied by $\alpha$ and we know $C$ is satisfied by $\astar$. Hence, at least one of the literals in $C$ does not comply with $\astar$. Now instead of guessing randomly which is the right literal to flip as in the random walk algorithm, we try all possibilities and at least one of them would be a good move, giving us $\dist{\astar}{\alpha^{[l = 1]}} = r - 1$. Moreover, if we make a good move, we do not need to change the assignment of that literal in the future. We can then assign that value permanently and search using $F^{[l=1]}$ and $\alpha^{[l=1]}$. Hence, this is a depth-first-search algorithm that finds the "lucky" path from $\alpha$ to $\astar$.
\end{proof}

\subsection{Derandomization Using Covering Code}
\paragraph{} Next, we derandomize the choices of initial assignment $\alpha$ with Covering Code. \par 
\begin{definition}[Covering Code]
	A Covering Code of length $n$, $\cc_r \subseteq \bits^n$ is defined as $\forall \alpha \in \bits^n, \exists c \in \cc_r$ such that $\alpha \in \ball{r}{c}$. That is, the Hamming Balls of radius $r$ centered at each member of $\cc_r$ covers the set $\bits^n$.
\end{definition}
\begin{lemma}[Bounding the volume of a Hamming Ball of radius $r$]
	\begin{align*}
	V(r) = \sum_{i = 0}^{n} {n \choose i}
	\end{align*}
	Let $\rho = r/n$. If $0 < \rho \leq 1/2$, then $V(r)$ can be estimated using: [citation needed]
	\begin{align*}
	\frac{1}{\sqrt{8n\rho(1-\rho)}} \cdot 2^{h(\rho)n} \leq V(r) \leq 2^{h(\rho)n}
	\end{align*}
	where $h(\rho) = -\rho \log_2 \rho - (1-\rho)\log_2(1-\rho)$ is the binary entropy function.
\end{lemma}
\begin{theorem}
	There exists a covering code $\cc_r$ of length $n$ and its size $|\cc_r|$ is at most $\lceil n\cdot 2^n / V(r) \rceil$.
\end{theorem}
\begin{proof}
	We give a probabilistic argument. Suppose we choose $ n\cdot 2^n / V(r)$ assignments from $\bits^n$ uniformly and independently to form $\cc_r$. Now fixed an assignment $\alpha$. The probability that $\alpha$ is covered by the Hamming Ball centered at a randomly chosen member $c \in \cc_r$ is exactly $V(r)/2^n$. 
	\begin{align*}
	\Pr(\text{$\alpha$ is not covered}) = \Big(1-\frac{V(r)}{2^n}\Big)^{n\cdot \frac{2^n}{V(r)}} \leq e^{-n}
	\end{align*}
	By taking a union bound, the probability that $\cc_r$ is a covering code $\geq 1 - 2^n\cdot e^{-n} > 0$. Hence, such a covering code exists. 
\end{proof}
Combining \textbf{Lemma 1} and \textbf{Theorem 4}, we have the following corollary:
\begin{corollary}
	Let $0 < \rho \leq 1/2$ and let $\beta(n) = \sqrt{8n\rho(1-\rho)}$. For all $n$, there exists a covering code $\cc$ of length $n$, radius $\rho n$ and size at most $n\beta(n)\cdot 2^{(1-h(\rho))n}$.
\end{corollary}
\begin{theorem}
	A covering code of length $n$ and size at most $n^2\beta(n)\cdot 2^{(1-h(\rho))n}$ can be constructed in $\O(2^{3n})$ time.
\end{theorem}
\begin{proof}
	Notice that constructing a covering code is indeed a set cover problem with $2^n$ sets and $2^n$ elements. There is a well-known greedy algorithm that gives a $\log(m)$-approximation[citation needed] to the set cover problem. In each iteration, we pick the Hamming Ball that covers the largest number of not-yet-covered elements. We can associate each Hamming Ball with the number of not-yet-covered elements that is covers. In each iteration, we pick a Hamming Ball greedily and update the numbers for all remaining Balls. This takes $\poly(n) \cdot 2^n \cdot 2^n = \poly(n) \cdot 2^{2n}$ time. There are at most $2^n$ iterations. Hence, the overall running time is $\O(2^{3n})$. \par 
	From \textbf{Corollary 1}, we know the optimal size of the covering code is at most $n\beta(n)\cdot 2^{(1-h(\rho))n}$. The algorithm above gives a $\log(m)$ approximation. Note that the size of the problem $m = 2^n$. Hence, the size of the covering code constructed is at most $\log(2^n) \cdot n\beta(n)\cdot 2^{(1-h(\rho))n} = n^2\beta(n)\cdot 2^{(1-h(\rho))n}$
\end{proof}
\begin{theorem}
	Let $d\geq 2$ be a divisor of $n$ and $0<\rho \leq 1/2$. Then a covering code of length $n$ and size at most $\O(2^{(1-h(\rho))n})$ can be constructed in time $\O(2^{3n/d}+ 2^{(1-h(\rho))n})$.
\end{theorem}
\begin{proof}
	Using \textbf{Theorem 5}, we can construct a covering code $\cc'$ of length $\frac{n}{d}$, radius $\frac{\rho n}{d}$. Now define $\cc = \{c_1c_2...c_d:c_1,c_2,...,c_d \in \cc'\}$ to be the covering code of length $n$, radius $\rho n$, that is, $\cc$ is the set of all concatenations of $d$ codes from $\cc'$.\par 
	It is easy to verify that $\cc$ is a covering code of length $n$. For any $\alpha \in \bits^n$, split $\alpha$ into $d$ blocks of length $\frac{n}{d}$ i.e. $\alpha = \alpha_1\alpha_2...\alpha_d$. For each $\alpha_i$, there exists a code $c_i \in \cc'$ such that $\dist{\alpha_i}{c_i} \leq \frac{\rho n}{d}$. Now the code $c = c_1c_2...c_d \in \cc$ and $\dist{\alpha}{c} \leq \rho n$. \par 
	The size of $\cc'$ is at most $n^2\beta(n)\cdot 2^{(1-h(\rho))n/d}$ and constructing it takes $\O(2^{3n/d})$ time. The size of $\cc = |\cc'|^d \leq n^{2d}\beta(n)^d\cdot 2^{(1-h(\rho))n} = \O(2^{(1-h(\rho))n})$. Hence, the time taken to construct $\cc$ is $\O(2^{3n/d}+ 2^{(1-h(\rho))n})$.
\end{proof}
\subsection{Overall Algorithm}
\paragraph{}Now that we have a covering code, we can complete our deterministic algorithm for $\SAT$:\par 
\begin{algorithm}[H]
	\caption{Deterministic $\SAT$}
	\DontPrintSemicolon
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
	\Input{A $\CNF$ formula $F$ with $n$ variables}
	\Output{A satisfying assignment $\astar$ or NO if no $\astar$ are found}
	\BlankLine
	Initialization: Compute covering code $\cc$\\
	\ForEach{code $c \in \cc$}{
		$\astar$ = $\PBS$-Recursion($F,c,\rho n$)\\
		\lIf{$\astar$ satisfies $F$}{return $\astar$}
	}
	{return NO}
\end{algorithm}
It is easy to see that Deterministic $\SAT$ returns a satisfying assignment if $F$ is satisfiable. Consider a satisfying assignment $\astar$. By definition of the covering code, $\exists c \in \cc$ such that $\dist{c}{\astar} \leq \rho n$. Hence, the depth-first-search $\PBS$-Recursion($F,c,\rho n$) would be able to find $\astar$. \par
The running time $T(n,\rho,d)$ involves computing the covering code,  $\O(2^{3n/d}+ 2^{(1-h(\rho))n})$ and doing the search, $\O(2^{(1-h(\rho))n})\cdot \O(3^{\rho n})$. That is,
\begin{align*}
T(n,\rho,d) = \O(2^{3n/d}+ 2^{(1-h(\rho))n}) + \O(2^{(1-h(\rho))n})\cdot \O(3^{\rho n})
\end{align*} 
It is easy to see that for a reasonably large $d$ such as $d \geq 6$, the first term is completely dominated by the second term. By differentiation we find $\rho = 1/4$ minimized the running time,
\begin{align*}
T(n,\rho,d) &=\O(2^{(1-h(\rho))n} \cdot 3^{\rho n})\\
&= \O\Big(2^{(1+ \frac{1}{4}\log_2\frac{1}{4} + \frac{3}{4}\log_2\frac{3}{4})n} \cdot 2^{(\frac{1}{4} \log_2 3 )n}\Big)\\
&= \O\Big(2^{(1 - \frac{1}{4}\log_2 4 + \frac{3}{4}\log_2 3 - \frac{3}{4}\log_2 4 + \frac{1}{4} \log_2 3)n}\Big)\\
&= \O\Big(2^{(1 - \log_2 4 + \log_2 3)n}\Big)\\
&= \O\big(\big(2 \cdot \frac{1}{4} \cdot 3\big)^{n}\big) = \O(1.5^n)
\end{align*} 
Notice that the performance is worse than the randomized algorithm and we will see how to improve it in the next section. In particular, if we can improve the performance of $\PBS$-Recursion, we arrive at the desired time complexity.
\section{Improving on the Derandomization\cite{Moser11}}
%k(3)-nary Covering Code
%Faster Recursion
%Overall time

\begin{thebibliography}{9}
\bibitem{Dantsin02}
E. Dantsin, A. Goerdt, E. A. Hirsch, R. Kannan, J. Kleinberg, C. Papadimitriou, O. Raghavan, and U. Sch\"{o}ning. 
\textit{A deterministic $(2-2/(k+1))n$ algorithm for k-SAT based on local search.} Theoretical Computer Science 289, (2002).

\bibitem{Moser11}
R. Moser and D. Scheder.
\textit{A Full Derandomization of Sch\"{o}ning’s k-SAT Algorithm.} Proceedings of the 44th Annual ACM Symposium on Theory of Computing, (2011).

\bibitem{Schoning99}
U. Sch\"{o}ning.
\textit{A Probabilistic Algorithm for $k$-SAT and Constraint Satisfaction Problems.} Proceedings of the 40th Annual Symposium on Foundations of Computer Science, (1999).

\end{thebibliography}

\end{document}

