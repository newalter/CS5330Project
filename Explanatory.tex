\documentclass[a4paper,12pts]{article}

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}

\newcommand{\SAT}{\textnormal{$k$-SAT}}
\newcommand{\SATbf}{\textbf{$k$-SAT}}
\newcommand{\CNF}{\textnormal{$k$-CNF}}
\newcommand{\vbl}[1]{\textnormal{vbl(#1)}}
\newcommand{\dist}[2]{d_H(#1,#2)}
\newcommand{\ball}[2]{B_{#1}(#2)}
\newcommand{\astar}{\alpha^*}
\newcommand{\PBS}{\textnormal{Promise-Ball-$\SAT$}}
\newcommand{\PBSbf}{\textbf{Promise-Ball-$\SATbf$}}
\newcommand{\cc}{\mathcal{C}}
\newcommand{\bits}{\{0,1\}}
\newcommand{\poly}{\textnormal{poly}}
\renewcommand{\Pr}{\textnormal{Pr}}
\renewcommand{\O}{\mathcal{O}^*}

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem*{note}{Note}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}


\title{Explanatory Report on Full Derandomization of Sch\"{o}ning's $\SAT$ Algorithm}
\author{}
\date{}

\begin{document}

\maketitle

\section{Introduction}
\paragraph{} $\SAT$ is a well-known and probably the most studied NP-Complete problem. In 1999, Uwe Sch\"{o}ning gave a simple yet efficient randomized algorithm for $\SAT$\cite{Schoning99}. The algorithm was later derandomized in two subsequent papers[\cite{Dantsin02,Moser11}]. In this explanatory report, we will reorganize their results and present the fully derandomized algorithm.\par 
The explanatory report is organized in the following manner. In section 2, we introduce some notations and definitions used in this paper. In section 3, we present Sch\"{o}ning's original randomized algorithm\cite{Schoning99} for $\SAT$, reorganized to facilitate our analysis for derandomization. In section 4, a derandomization algorithm based on local search by \cite{Dantsin02} is given. In section 5, we present an improvement by \cite{Moser11} that improves the derandomization algorithm and attains performance arbitrarily close to the randomized version, thereby claiming a full derandomization.

\section{Preliminaries}
\paragraph{} An input instance of a $\SAT$ problem is a $\CNF$(Conjunctive Normal Form) formula $F$ with $n$ variables. $F$ contains a finite set of clauses where each clause $C$ contains at most $k$ pairwise independent literals. A literal $u$ is either a variable $x$ or a complemented variable $\bar{x}$. Let $V$ denote the set of variables in $F$. \par 
A truth assignment $\alpha: V\rightarrow \{0,1\}$ assigns each variable a boolean value. If a literal $u = x$, it is satisfied by $\alpha$ iff $\alpha(x) = 1$. If $u = \bar{x}$, it is satisfied by $\alpha$ iff $\alpha(x) = 0$. A clause $C$ is satisfied by $\alpha$ if at least one of its literals are satisfied by $\alpha$. A formula $F$ is satisfied by $\alpha$ if all clauses are satisfied by $\alpha$. \par 
Given two assignments $\alpha$ and $\beta$, the Hamming distance between them, denoted as $\dist{\alpha}{\beta}$, is defined to be the number of variables $x \in V$ where $\alpha(x) \neq \beta(x)$. A Hamming Ball centered at $\alpha$ with radius $r$ is defined to be $\ball{r}{\alpha} = \{\beta: \dist{\alpha}{\beta} \leq r\}$ that is, all assignments whose Hamming distance from $\alpha$ is less than or equals to $r$. For assignments of $n$ variables, let $V(r)$ denotes the volume of a Hamming Ball of radius $r$, that is, the number of assignments in the Hamming Ball.\par 
For an assignment $\alpha$ and a literal $l$, we use $\alpha^{[l = 1]}$ to denote the assignment that is derived from $\alpha$ after setting $l$ to be $1$. Similarly for a formula $F$, we use $F^{[l = 1]}$ to denote the new formula obtained after setting $l$ to be $1$. \par 
\begin{definition}[\PBS\cite{Moser11}]
	An instance of the $\PBS$ problem $(F,\alpha,r)$ consists of a $\CNF$ formula $F$, an assignment $\alpha$ for $F$ and a natural number $r$. With the promise that the Hamming Ball $\ball{r}{\alpha}$ contains a satisfying assignment for $F$. Find any satisfying assignment.
\end{definition}
The $\PBS$ problem is an important sub-problem for solving $\SAT$. 

\section{Sch\"{o}ning's $\SATbf$ Algorithm\cite{Schoning99}}
The randomized algorithm for $\SAT$ builds on a simple random walk algorithm for solving $\PBS$: \par 
\begin{algorithm}[H]
	\caption{$\PBS$-Random-Walk}
	\DontPrintSemicolon
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
	\Input{$(F,\alpha,r)$}
	\Output{A satisfying assignment $\astar$ or NO if no $\astar$ are found}
	\BlankLine
	\For{steps = 1 to 3n}{
		\lIf{$\alpha$ satisfies $F$}{return $\alpha$}
		Choose an arbitrary clause $C$ not satified by $\alpha$\\
		Choose a literal in $C$ uniformly at random and flip its value in $\alpha$
	}
	\lIf{$\alpha$ satisfies $F$}{return $\alpha$}
	\hspace{7em}\lElse{return NO}
\end{algorithm}

\begin{theorem}[\cite{Schoning99}]
For a satisfying assignment $\astar \in \ball{r}{\alpha}$, where $\dist{\alpha}{\astar} = d \leq r$, $\PBS \textnormal{-Random-Walk}$ returns $\astar$ with probability $\geq (\frac{1}{k-1})^d$.
\end{theorem}
\begin{proof}[Proof Sketch]
	Let $q_d$ be the probability that $\PBS$-Random-Walk returns a satisfying assignment for $F$ given $\dist{\astar}{\alpha} = d$. In each step, with probability $\geq \frac{1}{k}$ the assignment $\alpha$ goes one step closer to $\astar$ and with probability $\leq 1 - \frac{1}{k}$ it goes one step further from $\astar$. Hence, $q_d \geq $ the probability that the random walk starting at position $r$ reaches position $0$ within $3n$ steps. Detailed analysis of the random walk gives $q_d \geq (\frac{1}{k-1})^d$.
\end{proof}

%Do we need this?
\iffalse
\begin{note}
	The input parameter $r$ is not used in $\PBS$\textnormal{-Random-Walk}. By nature of randomness, $\PBS$\textnormal{-Random-Walk} solves $(F,\alpha,d)$ with probability $\geq (\frac{1}{k-1})^{d}$.
\end{note}
\fi

With $\PBS$-Random-Walk, we have a randomized algorithm for $\SAT$:\par 
\begin{algorithm}[H]
	\caption{Randomized $\SAT$}
	\DontPrintSemicolon
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
	\Input{A $\CNF$ formula $F$ with $n$ variables}
	\Output{A satisfying assignment $\astar$ or NO if no $\astar$ are found}
	\BlankLine
	\For{iterations = 1 to m}{
		Choose a assignment $\alpha$ for $F$ uniformly at random\\
		$\astar$ = $\PBS$-Random-Walk($F,\alpha,n$)\\
		\lIf{$\astar$ satisfies $F$}{return $\astar$}
	}
	{return NO}
\end{algorithm}
\begin{theorem}
	If $F$ is satisfiable, each iteration of the for-loop in \textnormal{Randomized-}$\SAT$ returns a satisfying assignment with probability $\geq (\frac{k}{2(k-1)})^n$
\end{theorem}
\begin{proof}
	Fix a satisfying assignment $\astar$ for $F$, for the random initial assignment $\alpha$, $\Pr(\dist{\astar}{\alpha}=d) = {n \choose d}{2}^{-n}$. With \textbf{Theorem 1}, we have:
	\begin{align*}
	&\Pr(\textnormal{Success in an iteration}) 
	\geq \sum_{d=0}^{n} {n \choose d} 2^{-n} \cdot \big(\frac{1}{k-1}\big)^{d} 
	\geq \big(\frac{1}{2}\big)^{n} (1 + \frac{1}{k-1}) ^n = \big(\frac{k}{2(k-1)}\big)^{n}
	\end{align*}
\end{proof}
We can then amplify the probability by setting the number of iterations $m = \O((\frac{2(k-1)}{k})^n) = \O\big((2(1-\frac{1}{k}))^n\big)$(we use $\O$ to suppress the polynomial factor in $n$). Each iteration clearly runs in polynomial time and the overall runtime for the randomized algorithm is $\O\big((2(1-\frac{1}{k}))^n\big)$.
\section{Derandomization with Local Search\cite{Dantsin02}}
\paragraph{} Notice that in the randomized algorithm, randomness is used in two parts. One is in $\PBS$-Random-Walk and the other one is to find an assignment $\alpha$ that serves as the center of Hamming Ball $\ball{r}{\alpha}$. Both of them needs to be derandomized. \par 
\subsection{Derandomization of {$\PBSbf$}}
First of all, we derandomize $\PBS$-Random-Walk with a simple recursion algorithm. \par 
\begin{algorithm}[H]
	\caption{$\PBS$-Recursion}
	\DontPrintSemicolon
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
	\Input{$(F,\alpha,r)$}
	\Output{A satisfying assignment $\astar$ or NO if no $\astar$ are found}
	\BlankLine
	\lIf{$\alpha$ satisfies $F$}{return $\alpha$}
	\hspace{7em}\lElseIf{r = 0}{return NO}
	Choose an arbitrary clause $C$ not satified by $\alpha$\\
	\ForEach{literal $l \in C$}{
		$\alpha'$ = $\PBS$-Recursion($F^{[l = 1]}, \alpha^{[l=1]},r-1)$\\
		\lIf {$\alpha'$ satisfies $F$}{Return $\alpha'$}
	}
	{return NO}
\end{algorithm}
\begin{theorem}
	$\PBS \textnormal{-Recursion}$ solves $\PBS$ in $\O(k^r)$ time.
\end{theorem}
\begin{proof}
	The running time is straightforward. Each clause has at most $k$ literals and hence each node in the recursion tree has at most $k$ branches. The depth of the recursion three is at most $r$. Hence, there are at most $k^r$ leaves in the recursion tree. The overall runtime is $\O(k^r)$. \par 
	The correctness can be seen by drawing analogy to the random walk algorithm. Let $\astar$ be the satisfying assignment such that $\dist{\astar}{\alpha} \leq r$. Then there exists a ``lucky" random walk from $\alpha$ to $\astar$ that takes $\leq r$ steps, that is, in each step we move one step closer to $\astar$ and never makes a wrong move. Notice that if we never makes a wrong move, we will never change the assignment of a variable for more than once. \par 
	Now consider the recursion algorithm, in each recursive step we choose a clause $C$ not satisfied by $\alpha$ and we know $C$ is satisfied by $\astar$. Hence, at least one of the literals in $C$ does not comply with $\astar$. Now instead of guessing randomly which is the right literal to flip as in the random walk algorithm, we try all possibilities and at least one of them would be a good move, giving us $\dist{\astar}{\alpha^{[l = 1]}} = r - 1$. Moreover, if we make a good move, we do not need to change the assignment of that literal in the future. We can then assign that value permanently and search using $F^{[l=1]}$ and $\alpha^{[l=1]}$. Hence, this is a depth-first-search algorithm that finds the "lucky" path from $\alpha$ to $\astar$.
\end{proof}

\begin{note}
	$\PBS$-Recursion is much faster than enumerating all elements in the Hamming Ball because we are indeed performing an informed search. We only look at unsatisfied clauses and can hence prune out many unnecessary branches.
\end{note}
\subsection{Derandomization Using Covering Code}
\paragraph{} Next, we derandomize the choices of initial assignment $\alpha$ with Covering Code. \par 
\begin{definition}[Covering Code]
	A Covering Code of length $n$, $\cc_r \subseteq \bits^n$ is defined as $\forall \alpha \in \bits^n, \exists c \in \cc_r$ such that $\alpha \in \ball{r}{c}$. That is, the Hamming Balls of radius $r$ centered at each member of $\cc_r$ that cover the set $\bits^n$.
\end{definition}
\begin{lemma}[Bounding the volume of a Hamming Ball of radius $r$]
	\begin{align*}
	V(r) = \sum_{i = 0}^{n} {n \choose i}
	\end{align*}
	Let $\rho = r/n$. If $0 < \rho \leq 1/2$, then $V(r)$ can be estimated using: [citation needed]
	\begin{align*}
	\frac{1}{\sqrt{8n\rho(1-\rho)}} \cdot 2^{h(\rho)n} \leq V(r) \leq 2^{h(\rho)n}
	\end{align*}
	where $h(\rho) = -\rho \log_2 \rho - (1-\rho)\log_2(1-\rho)$ is the binary entropy function.
\end{lemma}
\begin{theorem}
	There exists a covering code $\cc_r$ of length $n$ and its size $|\cc_r|$ is at most $\lceil n\cdot 2^n / V(r) \rceil$.
\end{theorem}
\begin{proof}
	We give a probabilistic argument. Suppose we choose $ n\cdot 2^n / V(r)$ assignments from $\bits^n$ uniformly and independently to form $\cc_r$. Now fixed an assignment $\alpha$. The probability that $\alpha$ is covered by the Hamming Ball centered at a randomly chosen member $c \in \cc_r$ is exactly $V(r)/2^n$. 
	\begin{align*}
	\Pr(\text{$\alpha$ is not covered}) = \Big(1-\frac{V(r)}{2^n}\Big)^{n\cdot \frac{2^n}{V(r)}} \leq e^{-n}
	\end{align*}
	By taking a union bound, the probability that $\cc_r$ is a covering code $\geq 1 - 2^n\cdot e^{-n} > 0$. Hence, such a covering code exists. 
\end{proof}
Combining \textbf{Lemma 1} and \textbf{Theorem 4}, we have the following corollary:
\begin{corollary}
	Let $0 < \rho \leq 1/2$ and let $\beta(n) = \sqrt{8n\rho(1-\rho)}$. For all $n$, there exists a covering code $\cc$ of length $n$, radius $\rho n$ and size at most $n\beta(n)\cdot 2^{(1-h(\rho))n}$.
\end{corollary}
\begin{theorem}
	A covering code of length $n$ and size at most $n^2\beta(n)\cdot 2^{(1-h(\rho))n}$ can be constructed in $\O(2^{3n})$ time.
\end{theorem}
\begin{proof}
	Notice that constructing a covering code is indeed a set cover problem with $2^n$ sets and $2^n$ elements. There is a well-known greedy algorithm that gives a $\log(m)$-approximation[citation needed] to the set cover problem. In each iteration, we pick the Hamming Ball that covers the largest number of not-yet-covered elements. We can associate each Hamming Ball with the number of not-yet-covered elements that is covers. In each iteration, we pick a Hamming Ball greedily and update the numbers for all remaining Balls. This takes $\poly(n) \cdot 2^n \cdot 2^n = \poly(n) \cdot 2^{2n}$ time. There are at most $2^n$ iterations. Hence, the overall running time is $\O(2^{3n})$. \par 
	From \textbf{Corollary 1}, we know the optimal size of the covering code is at most $n\beta(n)\cdot 2^{(1-h(\rho))n}$. The algorithm above gives a $\log(m)$ approximation. Note that the size of the problem $m = 2^n$. Hence, the size of the covering code constructed is at most $\log(2^n) \cdot n\beta(n)\cdot 2^{(1-h(\rho))n} = n^2\beta(n)\cdot 2^{(1-h(\rho))n}$
\end{proof}
\begin{theorem}
	Let $d\geq 2$ be a divisor of $n$ and $0<\rho \leq 1/2$. Then a covering code of length $n$ and size at most $\O(2^{(1-h(\rho))n})$ can be constructed in time $\O(2^{3n/d}+ 2^{(1-h(\rho))n})$.
\end{theorem}
\begin{proof}
	Using \textbf{Theorem 5}, we can construct a covering code $\cc'$ of length $\frac{n}{d}$, radius $\frac{\rho n}{d}$. Now define $\cc = \{c_1c_2...c_d:c_1,c_2,...,c_d \in \cc'\}$ to be the covering code of length $n$, radius $\rho n$, that is, $\cc$ is the set of all concatenations of $d$ codes from $\cc'$.\par 
	It is easy to verify that $\cc$ is a covering code of length $n$. For any $\alpha \in \bits^n$, split $\alpha$ into $d$ blocks of length $\frac{n}{d}$ i.e. $\alpha = \alpha_1\alpha_2...\alpha_d$. For each $\alpha_i$, there exists a code $c_i \in \cc'$ such that $\dist{\alpha_i}{c_i} \leq \frac{\rho n}{d}$. Now the code $c = c_1c_2...c_d \in \cc$ and $\dist{\alpha}{c} \leq \rho n$. \par 
	The size of $\cc'$ is at most $n^2\beta(n)\cdot 2^{(1-h(\rho))n/d}$ and constructing it takes $\O(2^{3n/d})$ time. The size of $\cc = |\cc'|^d \leq n^{2d}\beta(n)^d\cdot 2^{(1-h(\rho))n} = \O(2^{(1-h(\rho))n})$. Hence, the time taken to construct $\cc$ is $\O(2^{3n/d}+ 2^{(1-h(\rho))n})$.
\end{proof}
\subsection{Combining the Derandomization}
\paragraph{}Now that we have a covering code, we can complete our deterministic algorithm for $\SAT$:\par 
\begin{algorithm}[H]
	\caption{Deterministic $\SAT$}
	\DontPrintSemicolon
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
	\Input{A $\CNF$ formula $F$ with $n$ variables}
	\Output{A satisfying assignment $\astar$ or NO if no $\astar$ are found}
	\BlankLine
	Initialization: Compute covering code $\cc$\\
	\ForEach{code $c \in \cc$}{
		$\astar$ = $\PBS$-Recursion($F,c,\rho n$)\\
		\lIf{$\astar$ satisfies $F$}{return $\astar$}
	}
	{return NO}
\end{algorithm}
It is easy to see that Deterministic $\SAT$ returns a satisfying assignment if $F$ is satisfiable. Consider a satisfying assignment $\astar$. By definition of the covering code, $\exists c \in \cc$ such that $\dist{c}{\astar} \leq \rho n$. Hence, the depth-first-search $\PBS$-Recursion($F,c,\rho n$) would be able to find $\astar$. \par
The running time $T(n,\rho,d)$ involves computing the covering code,  $\O(2^{3n/d}+ 2^{(1-h(\rho))n})$ and doing the search, $\O(2^{(1-h(\rho))n})\cdot \O(k^{\rho n})$. That is,
\begin{align*}
T(n,\rho,d) = \O(2^{3n/d}+ 2^{(1-h(\rho))n}) + \O(2^{(1-h(\rho))n})\cdot \O(k^{\rho n})
\end{align*} 
It is easy to see that for a reasonably large $d$ such as $d \geq 6$, the first term is completely dominated by the second term. By taking partial derivatives about $\rho$ on the exponential term, we can easily find out $\rho = 1/(k+1)$ minimized the running time. Hence, 
\begin{align*}
T(n,\rho,d) &=\O(2^{(1-h(\rho))n} \cdot k^{\rho n})\\
&= \O\Big(2^{(1+ \frac{1}{k+1}\log_2\frac{1}{k+1} + \frac{k}{k+1}\log_2\frac{k}{k+1})n} \cdot 2^{(\frac{1}{k+1} \log_2 k )n}\Big)\\
&= \O\Big(2^{(1 - \frac{1}{k+1}\log_2 (k+1) + \frac{k}{k+1}\log_2 k - \frac{k}{k+1}\log_2 (k+1) + \frac{1}{k+1} \log_2 k)n}\Big)\\
&= \O\Big(2^{(1 - \log_2 (k+1) + \log_2 k)n}\Big)\\
&= \O\big(\big(2 \cdot \frac{k}{k+1}\big)^{n}\big) = \O\big((2(1 - \frac{1}{k+1}))^n\big)
\end{align*} 
Notice that the performance is worse than the randomized algorithm and we will see how to improve it in the next section. In particular, we observe that the parameter $k$ in the overall running time comes from the running time of $\PBS$-Recursion, $\O(k^{\rho n})$. If we can improve its performance to $\O((k-1)^{\rho n})$, we arrive at a $\O\big((2(1 - \frac{1}{k}))^n\big)$ running time which is on par with the randomized algorithm.
\section{Improving on the Derandomization\cite{Moser11}}
\paragraph{} Notice that in the previous section, instead of searching the whole space of possible assignments, we cover the whole space with a large number of Hamming Balls and search in each Hamming Ball. The same idea can be used when searching in a Hamming Ball. In particular, we try to cover a large Hamming Ball with smaller Hamming Balls. By repeating this process, we arrive at a more efficient algorithm for $\PBS$. First of all, we look at $k$-nary covering code which is similar to the binary covering code in section 4.2.
\subsection{k-ary Covering Code}
%k-nary Covering Code
\subsection{Faster $\PBSbf$ Algorithm}
Using the previous tool, we are now ready to present the derandomised version of Sch\"{o}ning's algorithm. At first, before doing anything else, the algorithm will firstly compute a code $\mathcal{C} \subseteq {1, ...,k}^t$, where each element $c \in mathcal{C}$ has a radius of $\frac{t}{k}$, where $t$ is a some constant.\par

%% Mention the size of the resulting code here.

Upon the input of some k-CNF, the algorithm greedily tries to find the maximam set of clauses that are independent of each other. Let's call this set $\mathcal{G} = \{\mathcal{C}_1, \mathcal{C}_2, ...,\mathcal{C}_m\}$. Now we can break the problem instance into 2 cases: \par

Base case: The number of independent clauses is less than $t$. In which case we enumerate all possible assignments (of which there are $2^{km}$) and then call algorithm 3 with that assignment, with a radius of $r$. Essentially, this is just a search around all the possible assignments of distance at most $r$ away. Since we've set this to only happen when the number of clauses is below a certain constant, we have the number of possible assignments, $2^{km}$ of them is actually also just a constant. And since each search is bounded by $\O(k^r)$ time, we have that the base case runs in the same amount of time, since the search is only called a constant number of times.\par

On the other hand, if the number of independent clauses is more than $t$. Then the algorithm should select a set $H \subseteq G$ such that $|H|$ is of size exactly $t$. 
%then show the decomp and what does it help. 

\begin{thebibliography}{9}
\bibitem{Dantsin02}
E. Dantsin, A. Goerdt, E. A. Hirsch, R. Kannan, J. Kleinberg, C. Papadimitriou, O. Raghavan, and U. Sch\"{o}ning. 
\textit{A deterministic $(2-2/(k+1))n$ algorithm for k-SAT based on local search.} Theoretical Computer Science 289, (2002).

\bibitem{Moser11}
R. Moser and D. Scheder.
\textit{A Full Derandomization of Sch\"{o}ningâ€™s k-SAT Algorithm.} Proceedings of the 44th Annual ACM Symposium on Theory of Computing, (2011).

\bibitem{Schoning99}
U. Sch\"{o}ning.
\textit{A Probabilistic Algorithm for $k$-SAT and Constraint Satisfaction Problems.} Proceedings of the 40th Annual Symposium on Foundations of Computer Science, (1999).

\end{thebibliography}

\end{document}

